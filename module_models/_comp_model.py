from langchain_ollama import ChatOllama

llm = ChatOllama(model="llama3.2")

result = llm.invoke("What is LLM?")

print(result.content)

